import numpy as np
import matplotlib.pyplot as plt


class WindyGridworld:
    def __init__(self):
        # Grid dimensions: 7 rows x 10 columns
        self.rows = 7
        self.cols = 10

        # Wind strength for each column (upward wind)
        self.wind = np.array([0, 0, 0, 1, 1, 1, 2, 2, 1, 0])

        # Starting position: (3, 0)
        self.start_state = (3, 0)

        # Goal position: (3, 7)
        self.goal_state = (3, 7)

        # Actions: 0=up, 1=right, 2=down, 3=left
        self.actions = [(0, -1), (1, 0), (0, 1), (-1, 0)]
        self.action_names = ["left", "down", "right", "up"]
        self.num_actions = len(self.actions)

    def reset(self):
        # Reset to starting state
        return self.start_state

    def step(self, state, action):
        # Extract current position
        row, col = state

        # Get action movement
        d_col, d_row = self.actions[action]

        # Apply action
        new_row = row + d_row
        new_col = col + d_col

        # Apply wind effect (note that wind effect is upward, so it decreases row)
        new_row = new_row - self.wind[col]

        # Ensure we stay within grid boundaries
        new_row = max(0, min(new_row, self.rows - 1))
        new_col = max(0, min(new_col, self.cols - 1))

        # Create new state
        new_state = (new_row, new_col)

        # Reward is -1 for each step until reaching the goal
        reward = -1

        # Check if we've reached the goal
        done = new_state == self.goal_state

        return new_state, reward, done


def sarsa(env, episodes=500, alpha=0.5, gamma=1.0, epsilon=0.1):
    # Initialize Q-table with zeros
    Q = np.zeros((env.rows, env.cols, env.num_actions))

    # Store total rewards and steps per episode for plotting
    total_rewards = np.zeros(episodes)
    steps_per_episode = np.zeros(episodes)

    # Track cumulative time steps
    time_steps = []
    episode_numbers = []
    cumulative_steps = 0

    for episode in range(episodes):
        # Reset environment
        state = env.reset()
        total_reward = 0
        step_count = 0
        done = False

        # Choose initial action using epsilon-greedy
        if np.random.random() < epsilon:
            action = np.random.randint(env.num_actions)
        else:
            action = np.argmax(Q[state[0], state[1], :])

        # Continue until reaching the goal
        while not done:
            # Take action, observe new state and reward
            next_state, reward, done = env.step(state, action)
            total_reward += reward
            step_count += 1
            cumulative_steps += 1

            # Record episode number at each time step (for plotting)
            if episode > 0:  # Skip recording first few steps to make plot clearer
                time_steps.append(cumulative_steps)
                episode_numbers.append(episode)

            # Choose next action using epsilon-greedy
            if np.random.random() < epsilon:
                next_action = np.random.randint(env.num_actions)
            else:
                next_action = np.argmax(Q[next_state[0], next_state[1], :])

            # SARSA update
            Q[state[0], state[1], action] += alpha * (
                reward
                + gamma * Q[next_state[0], next_state[1], next_action]
                - Q[state[0], state[1], action]
            )

            # Update state and action for next iteration
            state = next_state
            action = next_action

        # Record metrics for this episode
        total_rewards[episode] = total_reward
        steps_per_episode[episode] = step_count

        # Print progress every 10 episodes
        if (episode + 1) % 10 == 0:
            print(
                f"Episode {episode+1}: Steps = {step_count}, Total Reward = {total_reward}"
            )

    return Q, total_rewards, steps_per_episode, time_steps, episode_numbers


def visualize_policy(env, Q):
    # Create a grid to visualize the policy
    policy = np.argmax(Q, axis=2)

    # Create a 2D grid of characters representing the policy
    policy_grid = np.full((env.rows, env.cols), " ", dtype=object)

    for row in range(env.rows):
        for col in range(env.cols):
            if (row, col) == env.start_state:
                policy_grid[row, col] = "S"
            elif (row, col) == env.goal_state:
                policy_grid[row, col] = "G"
            else:
                action = policy[row, col]
                if action == 0:  # left
                    policy_grid[row, col] = "←"
                elif action == 1:  # down
                    policy_grid[row, col] = "↓"
                elif action == 2:  # right
                    policy_grid[row, col] = "→"
                elif action == 3:  # up
                    policy_grid[row, col] = "↑"

    # Mark wind strengths at the bottom
    wind_str = ""
    for w in env.wind:
        wind_str += f" {w} "

    # Print the policy grid
    print("Policy Grid (S=Start, G=Goal):")
    for row in range(env.rows):
        print("|", end="")
        for col in range(env.cols):
            print(f" {policy_grid[row, col]} |", end="")
        print()

    print("\nWind strength per column:")
    print(wind_str)


def plot_episode_vs_timesteps(time_steps, episode_numbers):
    plt.figure(figsize=(10, 6))
    plt.scatter(time_steps, episode_numbers, alpha=0.5, s=5)

    # Add a best-fit line to show the trend
    if len(time_steps) > 1:
        z = np.polyfit(time_steps, episode_numbers, 1)
        p = np.poly1d(z)
        plt.plot(time_steps, p(time_steps), "r--", alpha=0.8)

    plt.ylabel("Episode Number")
    plt.xlabel("Time Steps")
    plt.title("Learning Progress: Episode vs Time Steps")
    plt.grid(True, alpha=0.3)

    # Add text annotation with the slope (learning rate)
    if len(time_steps) > 1:
        slope = z[0]
        plt.annotate(
            f"Slope: {slope:.6f} episodes/step",
            xy=(0.05, 0.95),
            xycoords="axes fraction",
            bbox=dict(boxstyle="round,pad=0.3", fc="white", ec="gray", alpha=0.8),
        )

    plt.tight_layout()
    plt.show()


def run_simulation():
    # Create environment
    env = WindyGridworld()

    # Train SARSA agent
    print("Training SARSA agent...")
    Q, total_rewards, steps, time_steps, episode_numbers = sarsa(
        env, episodes=170, alpha=0.5, gamma=1.0, epsilon=0.1
    )

    # Visualize the learned policy
    visualize_policy(env, Q)

    # Plot episode vs time steps
    plot_episode_vs_timesteps(time_steps, episode_numbers)

    # Print final statistics
    print(f"\nFinal performance (averaged over last 10 episodes):")
    print(f"Average steps: {np.mean(steps[-10:]):.2f}")
    print(f"Average reward: {np.mean(total_rewards[-10:]):.2f}")


if __name__ == "__main__":
    run_simulation()
